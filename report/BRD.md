# Business Requirements Document (BRD)
Project: Supermart Analytics Assignment
Author: Syed Shah
Version: 1.0
Date: <today>

## 1. Background
Supermart provides transactional sales data, item attributes, promotions, and store attributes. The client wants a reliable dataset, clear insights, and simple models to support promotion planning and store management.

## 2. Objectives
1) Clean and integrate raw datasets into a single master fact table.
2) Perform EDA to quantify trends, promotion uplift, and store performance.
3) Build supervised models for:
   - Sales forecasting (weekly, item × store).
   - High-performing store identification.
4) Deliver a concise business report with recommendations and a reproducible repo.

## 3. In Scope
- Data quality checks and standardization.
- Joins across Sales, Items, Promotions, Stores.
- Feature engineering: transaction_time, week/year, promo_flag, sales_per_unit.
- EDA visuals and summary tables.
- Baseline models (RandomForest regression; Logistic Regression classification).

## 4. Out of Scope
- Real-time pipelines or productionization.
- External data enrichment (weather, macro).
- Advanced hyperparameter tuning or MLOps.

## 5. Stakeholders
- Business sponsor
- Analytics/Marketing
- Data Engineering
- Store Operations

## 6. Data Sources
- Sales.csv — transactional facts.
- items.csv — item master data.
- Promotion.csv — promotion features/displays (by item/store/week where available).
- Supermarkets.csv — store attributes (province, etc.).

## 7. Business Rules & Transformations
- Standardize columns (lower snake_case; strip punctuation).
- Convert dates to `transaction_time`; derive `week` and `year`.
- Keys:
  - Item: `code`
  - Store: `supermarket_no` (auto-detected if named differently)
- Master table = Sales LEFT JOIN Items on `code`
                 LEFT JOIN Stores on `supermarket_no`
                 LEFT JOIN Promotion on `code + supermarket_no + week` (when available)
- Deduplicate each dataset.
- Guard divisions by zero when computing `sales_per_unit`.

## 8. KPIs
- Sales amount by month, province, store, and item.
- Promotion uplift (% change of mean sales during promo vs non-promo).
- Store revenue ranking; top-N items.
- Model metrics:
  - Forecasting: RMSE, R².
  - Classification: Precision/Recall/F1, ROC-AUC.

## 9. Acceptance Criteria
- Cleaned datasets and `cleaned_master.csv` generated by notebook 01.
- EDA notebook produces at least one time-trend chart and two tables (province revenue, top stores/items).
- Models notebook prints metrics without errors.
- Business report summarizes findings and recommendations.
- Repository is reproducible with README instructions.

## 10. Risks & Mitigation
- Inconsistent keys → auto-detection and renaming in cleaning.
- Large files in git → `.gitignore` excludes derived large CSVs; optional sample provided.
- Sparse/invalid dates → heuristic parsing; fallbacks to non-time splits.

## 11. Timeline & Milestones
- Day 1–2: Data cleaning and master join.
- Day 3: EDA and insights.
- Day 4: Modeling and evaluation.
- Day 5: Report and repo polish.

## 12. Deliverables
- `/notebooks/01_data_cleaning.ipynb`
- `/notebooks/02_exploratory_analysis.ipynb`
- `/notebooks/03_ml_models.ipynb`
- `/report/business_report.md` and exported figures/tables
- Updated `README.md` and `.gitignore`

## 13. Handover
Instructions to run locally are in README. Notebooks are self-contained and reproducible.
